ok, fuck, fine. an algorithm tester. a modular-ass algorithm tester. for anomalies. and/or anything else you might want to test. algorithms on.

1. write algorithm
2. test it on lots of data
3. write which data returns true and false
4. pre-classify data <- hard part. can be turked though.

I mean, Skyline essentially is an algorithm tester that gets run on lots of data. Why can't I use Skyline to iterate on algorithms? Because my data is not static. I need a collection of mad anomalous metrics. Once I have that, I can basically just run Skyline on it, right? I don't need Horizon or anything - I just need to load the data into Redis and fire away. I actually don't even need Redis. This is not hard, but it's just going to take a lot of work, setting up a garden of data with anomalies in it to test algorithms. The ideal algorithm will pick up the good ones and leave alone the bad ones.

1. Data loader to check if data is already in Redis - if not, load the data. We can just have load_data.py
2. An analyzer that multiprocesses like a motherfucker, drawing off a settings.py, and returns results. Ideally it returns a score - yes, this was supposed to be anomalous, and no, this wasn't supposed to be anomalous.
3. That's it.


1. For every datapoint in every metric, we need to specify beforehand whether it's supposed to be true or false.
2. ...mother of god. So let's just assume it's supposed to be false, and we will specify the individual datapoints that should trigger algorithms. Then, each algorithm will run through the entire series, and count the percentage of datapoints that it got right. After the metric is complete, the score will be finished for that algorithm. We don't need ensembles here, the idea is to test each and every algorithm by itself.

- slice the series, run the algorithm. continue until you've run algorithm on entire series iteratively, datapoint by datapoint, always testing if the last datapoint is anomalous.
- return red dot if false and supposed to be false, return green dot if true and supposed to be true. return P if true and supposed to be false, return N if false and supposed to be true.

Visual only? How the hell are we going to put this data together?
- with blood, sweat, and tears. It's gonna be boring as fuck but it is so motherfuckin' necessary.
- give me a break. By hand? That's not possible. I'm not going to sit here and edit 86400 datapoints.
- you don't have to. Set them all to false, look at the series, and set the ones you want to true. This is not hard. You will need a way to visualize them all in order to check them out and see which datapoints need to be set to true. If you're feeling ambitious, you can make an interactive editor that allows you to specify anomalies, without needing to edit the actual data file.
- a separate json file for each timeseries? I like that idea.

We'll have maybe 20 timeseries. That's 140 lines of output. Eh, doable.

UI:
crucible.1 :: .......................P.....N...NN.NN....P...PPP....NN...NNNNN.PPP...nnn..ppPP
    - use N for false negative, P for false positive.
    - use different capitalizations to indicate degree of wrongness
    - interpolate and sample the output so we don't print 86400 dots for each series
    - write locations of negative and positive to disk to be viewed in the context of the series, in the webapp.
    - red dots for detected normality, green dots for detected anomalies? Green Ps and red Ns?
